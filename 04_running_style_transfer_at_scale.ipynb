{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running style transfer at scale\n",
    "This notebook will take us through what it looks like to run neural style transfer at scale in Azure using Batch AI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import utilities. For this notebook, we're going to use some of the utilities provided as part of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from az.util import bai\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your cluster\n",
    "This section will walk through setting up your cluster and some of the parameters you can use to customize your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided utility, set up the Batch AI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bai.setup_bai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the cluster, we need to set up parameters for our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"<my-workspace>\"\n",
    "cluster_name = \"<my-cluster-name>\"\n",
    "cluster_vm_size = \"<vm-size>\"\n",
    "cluster_vm_priority = \"<vm-priority>\"\n",
    "cluster_minimum_node_count = \"<minimum-node-count>\"\n",
    "cluster_maximum_node_count = \"<maximum-node-count>\"\n",
    "cluster_initial_node_count = \"<initial-node-count>\"\n",
    "cluster_container_mnt_path = \"<container-mnt-path>\"\n",
    "admin_user_name = \"<username>\"\n",
    "admin_user_password = \"<my-secret-password>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cluster settings to the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set WORKSPACE $workspace\n",
    "!dotenv set CLUSTER_NAME $cluster_name\n",
    "!dotenv set CLUSTER_VM_SIZE $cluster_vm_size\n",
    "!dotenv set CLUSTER_VM_PRIORITY $cluster_vm_priority\n",
    "!dotenv set CLUSTER_MINIMUM_NODE_COUNT $cluster_minimum_node_count\n",
    "!dotenv set CLUSTER_MAXIMUM_NODE_COUNT $cluster_maximum_node_count\n",
    "!dotenv set CLUSTER_INITIAL_NODE_COUNT $cluster_initial_node_count\n",
    "!dotenv set CLUSTER_CONTAINER_MNT_PATH $cluster_container_mnt_path\n",
    "!dotenv set ADMIN_USER_NAME $admin_user_name\n",
    "!dotenv set ADMIN_USER_PASSWORD $admin_user_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Batch AI client, we can set up the cluster. In Batch AI, clusters must belong within a _workspace_. Next, create both the workspace and cluster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bai.create_workspace(client, ws=workspace)\n",
    "bai.create_autoscale_cluster(client, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the state of the cluster.\n",
    "\n",
    "NOTE: This can also be done by going into the portal and inspecting the cluster via the Batch AI UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = bai.get_cluster(client, cluster_name, ws=workspace)\n",
    "print(('Cluster state: {0}; Allocated: {1}; Idle: {2}; ' +\n",
    "     'Unusable: {3}; Running: {4}; Preparing: {5}; ' +\n",
    "     'Leaving: {6}').format(\n",
    "        cluster.allocation_state,\n",
    "        cluster.current_node_count,\n",
    "        cluster.node_state_counts.idle_node_count,\n",
    "        cluster.node_state_counts.unusable_node_count,\n",
    "        cluster.node_state_counts.running_node_count,\n",
    "        cluster.node_state_counts.preparing_node_count,\n",
    "        cluster.node_state_counts.leaving_node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a job on the cluster\n",
    "This section of the notebook will walk through how to run a Batch AI job on the cluster we just created. This section will first use AzCopy to upload our individual frames from the video onto blob storage. It will also use AzCopy to copy over the style transfer script and the style image.\n",
    "\n",
    "After that, we will primarily be relying on the script __create_job.py__ to create jobs on Batch AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use AzCopy to upload the following items into the storage account we've created:\n",
    "\n",
    "- The style image - \n",
    "- The style transfer script\n",
    "- The directory with all the individual frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_script = \"script.py\"\n",
    "fs_style_image = \"style_image.py\"\n",
    "fs_content_dir = \"orangutan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set FS_SCRIPT $fs_script\n",
    "!dotenv set FS_STYLE_IMAGE $fs_style_image\n",
    "!dotenv set FS_CONTENT_DIR $fs_content_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "azcopy \\\n",
    "    --source pytorch/style_transfer_script.py \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_SCRIPT} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY\n",
    "    \n",
    "azcopy \\\n",
    "    --source pytorch/images/style_images/sample_renior.jpg \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_STYLE_IMAGE} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY\n",
    "    \n",
    "azcopy \\\n",
    "    --source pytorch/images/${VIDEO_NAME} \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_CONTENT_DIR} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY \\\n",
    "    --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_prefix = \"exp\"\n",
    "job_name_prefix = \"job\"\n",
    "job_node_count = 1\n",
    "job_batch_size = 50\n",
    "fs_output_dir_prefix = \"output\"\n",
    "fs_logger_dir_prefix = \"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set EXPERIMENT_PREFIX $experiment_prefix\n",
    "!dotenv set JOB_NAME_PREFIX $job_name_prefix\n",
    "!dotenv set JOB_NODE_COUNT $job_node_count\n",
    "!dotenv set JOB_BATCH_SIZE $job_batch_size\n",
    "!dotenv set FS_OUTPUT_DIR_PREFIX $fs_output_dir_prefix\n",
    "!dotenv set FS_LOGGER_DIR_PREFIX $fs_logger_dir_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python az/create_job.py --job-batch-size 50 --content-images-blob-dir $fs_content_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the jobs finish running, you can use the Azure portal or Storage explorer to inspect the output images.\n",
    "\n",
    "Inside your Blob Container, you should notice that a new directory with the datetime-stamp is created. Output images are stored there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "[todo]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batchscoringdl]",
   "language": "python",
   "name": "conda-env-batchscoringdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
