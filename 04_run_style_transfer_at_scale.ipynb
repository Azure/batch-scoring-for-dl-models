{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run style transfer at scale\n",
    "This notebook will take us through what it looks like to run neural style transfer at scale in Azure using Batch AI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import utilities. For this notebook, we're going to use some of the utilities provided as part of this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from az.util import bai\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your cluster\n",
    "This section will walk through setting up your cluster and some of the parameters you can use to customize your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided utility, set up the Batch AI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bai.setup_bai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the cluster, we need to set up parameters for our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"<my-workspace>\"\n",
    "cluster_name = \"<my-cluster-name>\"\n",
    "cluster_vm_size = \"<vm-size>\"\n",
    "cluster_vm_priority = \"<vm-priority>\"\n",
    "cluster_minimum_node_count = \"<minimum-node-count>\"\n",
    "cluster_maximum_node_count = \"<maximum-node-count>\"\n",
    "cluster_initial_node_count = \"<initial-node-count>\"\n",
    "cluster_container_mnt_path = \"<container-mnt-path>\"\n",
    "admin_user_name = \"<username>\"\n",
    "admin_user_password = \"<my-secret-password>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cluster settings to the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set WORKSPACE $workspace\n",
    "!dotenv set CLUSTER_NAME $cluster_name\n",
    "!dotenv set CLUSTER_VM_SIZE $cluster_vm_size\n",
    "!dotenv set CLUSTER_VM_PRIORITY $cluster_vm_priority\n",
    "!dotenv set CLUSTER_MINIMUM_NODE_COUNT $cluster_minimum_node_count\n",
    "!dotenv set CLUSTER_MAXIMUM_NODE_COUNT $cluster_maximum_node_count\n",
    "!dotenv set CLUSTER_INITIAL_NODE_COUNT $cluster_initial_node_count\n",
    "!dotenv set CLUSTER_CONTAINER_MNT_PATH $cluster_container_mnt_path\n",
    "!dotenv set ADMIN_USER_NAME $admin_user_name\n",
    "!dotenv set ADMIN_USER_PASSWORD $admin_user_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Batch AI client, we can set up the cluster. In Batch AI, clusters must belong within a _workspace_. Next, create both the workspace and cluster: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bai.create_workspace(client, ws=os.getenv('WORKSPACE'))\n",
    "bai.create_autoscale_cluster(client, os.getenv('CLUSTER_NAME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the state of the cluster.\n",
    "\n",
    "NOTE: This can also be done by going into the portal and inspecting the cluster via the Batch AI UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = bai.get_cluster(client, name=os.getenv('CLUSTER_NAME'), ws=os.getenv('WORKSPACE'))\n",
    "print(('Cluster state: {0}; Allocated: {1}; Idle: {2}; ' +\n",
    "     'Unusable: {3}; Running: {4}; Preparing: {5}; ' +\n",
    "     'Leaving: {6}').format(\n",
    "        cluster.allocation_state,\n",
    "        cluster.current_node_count,\n",
    "        cluster.node_state_counts.idle_node_count,\n",
    "        cluster.node_state_counts.unusable_node_count,\n",
    "        cluster.node_state_counts.running_node_count,\n",
    "        cluster.node_state_counts.preparing_node_count,\n",
    "        cluster.node_state_counts.leaving_node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a job on the cluster\n",
    "This section of the notebook will walk through how to run a Batch AI job on the cluster we just created. This section will first use AzCopy to upload our individual frames from the video onto blob storage. It will also use AzCopy to copy over the style transfer script and the style image.\n",
    "\n",
    "After that, we will primarily be relying on the script `create_job.py` to create jobs on Batch AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define some variables for Azure storage (which we'll treat as our fileshare):\n",
    "\n",
    "- `fs_script` - what to name the script that is uploaded to storage\n",
    "- `fs_style_image` - what to name the style image that is uploaded to storage\n",
    "- `fs_content_dir` - what to name the directory that we'll store all the content images we want to apply style transfer to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_script = \"script.py\"\n",
    "fs_style_image = \"style_image.py\"\n",
    "fs_content_dir = \"orangutan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist these variables to our `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set FS_SCRIPT $fs_script\n",
    "!dotenv set FS_STYLE_IMAGE $fs_style_image\n",
    "!dotenv set FS_CONTENT_DIR $fs_content_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload our environment variables from our `.env` file so we can use these variables as enviroment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use AzCopy to upload the following items into the storage account we've created:\n",
    "\n",
    "- `style_transfer_script.py` - this is the style transfer script that we've already tested locally. We'll upload this script as `fs_script`\n",
    "- `sample_renior.jpg` - this is the style image that the style transfer script will use. We'll upload this script as `fs_style_image`\n",
    "- `/orangutan` - this is the directory with all the individual frames. We'll name the directory once uploaded to storage as `fs_content_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "azcopy \\\n",
    "    --source pytorch/style_transfer_script.py \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_SCRIPT} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY\n",
    "    \n",
    "azcopy \\\n",
    "    --source pytorch/images/style_images/sample_renior.jpg \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_STYLE_IMAGE} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY\n",
    "    \n",
    "azcopy \\\n",
    "    --source pytorch/images/${VIDEO_NAME} \\\n",
    "    --destination https://${STORAGE_ACCOUNT_NAME}.blob.core.windows.net/${AZURE_CONTAINER_NAME}/${FS_CONTENT_DIR} \\\n",
    "    --dest-key $STORAGE_ACCOUNT_KEY \\\n",
    "    --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the images and scripts that Batch AI will use uploaded to storage, we need to setup our Batch AI job. First we define some variables for the job:\n",
    "\n",
    "- `experiment_prefix` - experiments are a logical container for Batch AI jobs. This is the prefix for your experiment name.\n",
    "- `job_name_prefix` - this is the individual Batch AI job. this is the prefix for the jobs name.\n",
    "- `job_node_count` - this number defines the number of jobs to run on a single node.\n",
    "- `job_batch_size` - this number defines how many images to process on a single job.\n",
    "- `fs_output_dir_prefix` - all output images will be saved into a folder in blob. This variable is the prefix for the name of that folder.\n",
    "- `fs_logger_dir_prefix` - all jobs will output log files into a folder in blob. This variable is the prefix for the name of that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_prefix = \"exp\"\n",
    "job_name_prefix = \"job\"\n",
    "job_node_count = 1\n",
    "job_batch_size = 50\n",
    "fs_output_dir_prefix = \"output\"\n",
    "fs_logger_dir_prefix = \"log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist these variables to our `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dotenv set EXPERIMENT_PREFIX $experiment_prefix\n",
    "!dotenv set JOB_NAME_PREFIX $job_name_prefix\n",
    "!dotenv set JOB_NODE_COUNT $job_node_count\n",
    "!dotenv set JOB_BATCH_SIZE $job_batch_size\n",
    "!dotenv set FS_OUTPUT_DIR_PREFIX $fs_output_dir_prefix\n",
    "!dotenv set FS_LOGGER_DIR_PREFIX $fs_logger_dir_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload our environment variables from our `.env` file so we can use these variables as enviroment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `create_job.py` file, lets kick off our Batch AI jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python az/create_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the jobs finish running, you can use the Azure portal or Storage explorer to inspect the output images.\n",
    "\n",
    "Inside your Blob Container, you should notice that a new directory with the datetime-stamp is created. Output images are stored there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we've configured a bunch of settings for what the cluster should look like, how the files in storage should be structured, and what the job parameters should be. You can feel free to configure and play around with these parameters to make sure that they work best for your workload. \n",
    "\n",
    "With these parameters set, we've also created an autoscaling cluster in Batch AI, uploaded scripts and images into Azure blob storage, and used those scripts and images to perform a batch style transfer job at scale with the `create_job.py` script.\n",
    "\n",
    "Next, we're going to [setup our Docker image and run the Batch AI jobs from there.](./05_run_the_batch_ai_job_from_docker.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batchscoringdl]",
   "language": "python",
   "name": "conda-env-batchscoringdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
